import logging
from prefect import flow, tags
from toolkits.utils import *


@flow
def update_zarr_store(bucket: str, object_key: str, event_name: str):
    logger = get_run_logger()

    dataset = find_handler(object_key)
    store_path = dataset.get_store_path()
    logger.info(f"store_path: {store_path}")
    zarr_store = get_zarr_store(store_path)

    logger.info(f"reading new NetCDF file to xarray dataset")
    file_ds = read_netcdf(bucket, object_key, dataset.processor)

    if is_first_write(zarr_store):
        # Personal thought: even we can sort it out write safe/write consistency to address the incident above,
        # the big Zarr store of each data collection should be pre-generated by separate processes/pipelines
        # that utilising Dask and processing multiple files effectively (in chronologically order) at once etc.
        # It's fast, and a proven approach applied by many.

        # this update_zarr_region pipeline which processes 1 file per call should really just do two things:
        # 1. append new dataset to existing Zarr store if a NetCDF file hasn't been ingested.
        # 2. overwrite a region (with or without new data) of existing Zarr store if a NetCDF file already been ingested.

        raise ValueError("Zarr store is pre-generated, pipeline runs shouldn't reach here!")

        # logger.info(f"writing the NetCDF file to the Zarr store")
        # with tags('write new zarr store'):
        #     write_zarr(zarr_store, file_ds)
    else:
        region = dataset.get_zarr_region(zarr_store, file_ds)
        if region is not None:
            if event_name == 'ObjectCreated':
                logger.info(f"overwriting zarr store region by new NetCDF file")
                tag = 'overwrite with revised data'
            else:
                logger.info(f"removing NetCDF file contents from existing Zarr store")
                file_ds = dataset.generate_empty_ds(file_ds)
                tag = 'overwrite with empty chunks'
            with tags(tag):
                overwrite_zarr_region(zarr_store, file_ds, region)
        else:
            logger.info(f"appending NetCDF file to current Zarr store")
            append_dim = dataset.get_append_dim()
            with tags('append new data'):
                append_zarr(zarr_store, file_ds, append_dim)
    logger.info("completed!")


def lambda_handler(event, context):
    """
    Lambda's handler method that should keep method signature unchanged
    """
    aws_logger = logging.getLogger()
    aws_logger.setLevel(logging.INFO)

    aws_logger.info("Received event: " + json.dumps(event))

    # TODO: Uncomment below variable assignments if using SQS queue,
    #  there is a Lambda function called `UpdateZarrRegionS3Watch` that sends messages to the SQS queue
    #  I played around a bit, it didn't work pretty well when uploading lots of files.
    #  Question? can we trade off chronologically order when appending/writing Zarr store with multiple files?
    # data = json.loads(event['Records'][0]['body'])
    # bucket = data['bucket']
    # event_name = data['event_name']
    # object_key = data['object_key']

    bucket = event['Records'][0]['s3']['bucket']['name']
    tmp_event_name = event['Records'][0]['eventName']
    event_name = tmp_event_name.split(':')[0]

    if event_name == 'ObjectRemoved':
        # get_zarr_region() needs to read a NetCDF file to xarray dataset to run comparison logics
        # but when it is removed from raw bucket, it can't be read anymore
        # So, assuming 'imos-data' bucket keeps all the files even when it's removed from raw bucket.
        # TODO: consider adding objectKey as filename in data_variables when creating the Zarr store
        #  so we can use objectKey for finding region instead,
        #  but what happens if revised file doesn't come with the original name?
        #  eg. 20220113032000-ABOM-L3S_GHRSST-SSTskin-AVHRR_D-1d_day.nc -> Revised_20220113032000-ABOM-L3S_GHRSST-SSTskin-AVHRR_D-1d_day.nc
        bucket = 'imos-data'

    object_key = event["Records"][0]["s3"]["object"]["key"]

    with tags(object_key, event_name, 'email_on_failure'):
        update_zarr_store(bucket, object_key, event_name)


# FOR LOCAL DEV
if __name__ == '__main__':
    bucket = 'imos-data'
    # object_key = 'IMOS/SRS/SST/ghrsst/L3S-1d/day/2022/20220317032000-ABOM-L3S_GHRSST-SSTskin-AVHRR_D-1d_day.nc'
    object_key = 'IMOS/SRS/SST/ghrsst/L3S-1d/day/2022/20220113032000-ABOM-L3S_GHRSST-SSTskin-AVHRR_D-1d_day.nc'
    # object_key = 'IMOS/SST/updated/out.nc'  # this file has doubled sea_surface_temperature values
    # bucket = 'vhnguyen'
    event_name = 'ObjectCreated'
    update_zarr_store(bucket, object_key, event_name)
